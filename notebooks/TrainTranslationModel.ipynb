{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPC_-fHYSt-4"
      },
      "source": [
        "# **Installing dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plZclkegSznl",
        "outputId": "0b16a0f0-55b0-4c05-95dd-f7e640c1e67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, onnx, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 onnx-1.16.1 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lwBIKwFQ_JD"
      },
      "source": [
        "# **Importing dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLRxJ_dGRD42"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, MarianConfig, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parallel import DataParallel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3r_zwZPRISQ"
      },
      "source": [
        "# **Loading Pretrained Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSrygRtWRJq3"
      },
      "outputs": [],
      "source": [
        "config = MarianConfig(\n",
        "    vocab_size=30000,\n",
        "    d_model=512,\n",
        "    encoder_layers=6,\n",
        "    decoder_layers=6,\n",
        "    encoder_attention_heads=8,\n",
        "    decoder_attention_heads=8,\n",
        "    encoder_ffn_dim=2048,\n",
        "    decoder_ffn_dim=2048,\n",
        "    pad_token_id=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qert0Mh0AvvY",
        "outputId": "947c3386-04d4-4820-d502-8400cdb5248c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "config = MarianConfig.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYQLCSXL_Ejj",
        "outputId": "305f6e88-0a5c-4ef4-c45b-2e25d36b8421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "model = MarianMTModel(config)\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSC6LLKFFjCz"
      },
      "source": [
        "This code i have used to get partitially trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YTiYMXOFiTB",
        "outputId": "3565c409-2ec2-4749-a538-fa59f510016c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = \"/content/drive/MyDrive/Text2Text3Epoch\"\n",
        "\n",
        "model_path = Path(folder_path) / \"translation_model\"\n",
        "tokenizer_path = Path(folder_path) / \"translation_tokenizer\"\n",
        "\n",
        "model = MarianMTModel.from_pretrained(model_path)\n",
        "tokenizer = MarianTokenizer.from_pretrained(tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W2RzKZvVxw2",
        "outputId": "8a03b8fe-fd9e-4391-f079-f49322eab3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6PS_itRKwe"
      },
      "source": [
        "# **Loading and Preprocessing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhcKZ2OLRfg_",
        "outputId": "a1b9e0d0-b5c1-4d91-e12b-6a8abaf30ee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ru\") # Download dataset from Hugging Face\n",
        "train_dataset = dataset[\"train\"] # Subdivide it in train.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9kWUBBRhOW"
      },
      "source": [
        "Creating the Translation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw45bvZyRjzY"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        ''' Initialize dataset '''\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Return length of dataset '''\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "         This function will be used\n",
        "         in training model to get\n",
        "         batches\n",
        "         '''\n",
        "        example = self.dataset[idx] # Get items by indexes\n",
        "        source_text = example[\"translation\"][\"en\"] # Source language - english\n",
        "        target_text = example[\"translation\"][\"ru\"] # and translation - russian\n",
        "\n",
        "        # Tokenize and encode the text\n",
        "        inputs = tokenizer.prepare_seq2seq_batch(\n",
        "            [source_text], # Tokenize english text\n",
        "            truncation=True, # Truncation\n",
        "            padding=\"max_length\", # Pad by longest item\n",
        "            max_length=256, # Max length of sequence\n",
        "            return_tensors=\"pt\", # And return PyTorch Tensors\n",
        "        )\n",
        "        input_ids = inputs.input_ids.squeeze() # Squeeze text dimension\n",
        "        attention_mask = inputs.attention_mask.squeeze() # Squeeze Attention-mask\n",
        "        labels = tokenizer.prepare_seq2seq_batch( # And now make the same with russian texts\n",
        "            [target_text], # Russian text\n",
        "            truncation=True, # Truncation\n",
        "            padding=\"max_length\", # Pad by longest item\n",
        "            max_length=256, # Max lenght of sequence\n",
        "            return_tensors=\"pt\", # Return PyTorch tensors\n",
        "        ).input_ids.squeeze() # Squeese dimension\n",
        "\n",
        "        return { # And return results\n",
        "            \"input_ids\": input_ids, # English text\n",
        "            \"attention_mask\": attention_mask, # Attention-mask\n",
        "            \"labels\": labels, # Russian text\n",
        "        }\n",
        "\n",
        "train_dataset = TranslationDataset(train_dataset) # And here we create TD objects for train.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIO4aNMKRlZ2"
      },
      "source": [
        "# **Creating Data Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE-wPbvcRnwF"
      },
      "outputs": [],
      "source": [
        "# Here we are turning our custom dataset to DataLoader's\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=12) # Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENREXVOxB75W"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csS80ax2Wv4E",
        "outputId": "b8439847-9217-449c-e009-01f86ca558f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RhNmXGRsBB"
      },
      "source": [
        "# **Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PsaO_2PwRsmW",
        "outputId": "98142f3e-ec09-49a9-dad7-b7711a017069"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "num_epochs = 1 # amount of training epochs\n",
        "learning_rate = 1e-3 # speed of learning (learning rate)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate) # Create Optimizer for model\n",
        "\n",
        "model.train() # Switch model to train-mode\n",
        "\n",
        "model = DataParallel(model).to(device)\n",
        "\n",
        "for epoch in range(num_epochs): # For epoch..\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch: {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar: # Take each batch from dataset\n",
        "        optimizer.zero_grad() # Clear gradients of optimizer\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device) # 'Unzip' our batch\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        # Get model predictions\n",
        "        loss = outputs.loss # Get losses\n",
        "\n",
        "        loss.backward() # Step\n",
        "        optimizer.step() # Back propogation\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item(), \"Avg Loss\": total_loss / (progress_bar.n + 1)})\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\") # Inform us about ended epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koSyfKp_Wvad",
        "outputId": "626fc321-4bd7-4689-bc27-2abda0faf76d"
      },
      "outputs": [],
      "source": [
        "# Токенизация предложения\n",
        "sentence = input()\n",
        "\n",
        "# Преобразование предложения в токены\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
        "\n",
        "# Генерация текста\n",
        "with torch.no_grad():\n",
        "    output = model.module.generate(input_ids)  # Вызов generate на модели, обращение к модели через .module\n",
        "\n",
        "# Преобразование вывода в текст\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Вывод результата\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKeIduI5RuoI"
      },
      "source": [
        "# **Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hN8j-XsRwHU",
        "outputId": "97f24e40-64ce-4750-8f57-05f5e18e9c71"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model on the test set\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "average_loss = total_loss / len(test_dataloader)\n",
        "print(f\"Test Loss: {average_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqKAqdz4Rx-9"
      },
      "source": [
        "# **Saving the Trained Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLkk2Az0BVbC"
      },
      "source": [
        "Deprecated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8haE-OEbau2",
        "outputId": "ce1fad72-89d5-4d9a-b941-d9f43455993b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1017: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Example\n",
        "text = \"Example for tokenizer.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "# Step 2: Encoding\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# Step 3: Adding attention mask\n",
        "attention_mask = [1] * len(input_ids)  # Example\n",
        "\n",
        "# Step 4: Converting to tensor\n",
        "input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
        "attention_mask = torch.tensor([attention_mask], dtype=torch.long)\n",
        "\n",
        "# Step 5: Loading to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "# Your actual decoder parameters\n",
        "decoder_input_ids = torch.tensor([[1, 2, 3]], dtype=torch.long)  # Example decoder token identifiers\n",
        "# or\n",
        "decoder_inputs_embeds = torch.tensor([[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]], dtype=torch.float)  # Example decoder input embeddings\n",
        "\n",
        "# Using the model with decoder parameters\n",
        "outputs = model(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    decoder_input_ids=decoder_input_ids,  # or decoder_inputs_embeds\n",
        ")\n",
        "\n",
        "# Save in ONNX Format\n",
        "torch.onnx.export(\n",
        "    model=model,\n",
        "    args=(input_ids, attention_mask, decoder_input_ids),  # or args=(input_ids, attention_mask, decoder_inputs_embeds)\n",
        "    f=\"model.onnx\",  # Path to ONNX-model\n",
        "    input_names=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\"],  # Names of inputs\n",
        "    output_names=[\"output\"],  # Names of outputs\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # Dynamic axis\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"decoder_input_ids\": {0: \"batch_size\", 1: \"decoder_length\"}  # Adjust the dynamic axis accordingly\n",
        "    },\n",
        "    opset_version=12  # Version\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKStwMrRBmNE",
        "outputId": "dbe9f824-a8e3-4aa5-e4bf-61d15b44fc07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62517]], 'forced_eos_token_id': 0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('translation_tokenizer/tokenizer_config.json',\n",
              " 'translation_tokenizer/special_tokens_map.json',\n",
              " 'translation_tokenizer/vocab.json',\n",
              " 'translation_tokenizer/source.spm',\n",
              " 'translation_tokenizer/target.spm',\n",
              " 'translation_tokenizer/added_tokens.json')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.module.save_pretrained(\"translation_model\")\n",
        "tokenizer.save_pretrained(\"translation_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TkyhBAPwdpC8",
        "outputId": "8749c0fa-47b2-4fce-d4db-25bb1952d5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Text2Text2Epoch/translation_model'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Path to model\n",
        "onnx_model_path = '/content/translation_model'\n",
        "\n",
        "# Destination path Google Drive\n",
        "destination_path = '/content/drive/MyDrive/Text2Text3Epoch/translation_model'\n",
        "\n",
        "# Copy model to Google Drive\n",
        "shutil.copytree(onnx_model_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5ncoxHkeXgmy",
        "outputId": "e1717f32-67a3-46c4-b312-81f193ac7b62"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Text2Text2Epoch/translation_tokenizer'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Path to tokenizer\n",
        "onnx_model_path = '/content/translation_tokenizer'\n",
        "\n",
        "# Destination path Google Drive\n",
        "destination_path = '/content/drive/MyDrive/Text2Text3Epoch/translation_tokenizer'\n",
        "\n",
        "# Copy tokenizer to Google Drive\n",
        "shutil.copytree(onnx_model_path, destination_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
